{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/openai/whisper.git\n",
    "\n",
    "!pip install torch openai-whisper\n",
    "\n",
    "# !pip install openai-whisper\n",
    "import whisper\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_path, model_name=\"base\"):\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(model_name)\n",
    "\n",
    "    # Perform the transcription\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # Extract the transcribed text\n",
    "    transcribed_text = result[\"text\"]\n",
    "\n",
    "    return transcribed_text\n",
    "\n",
    "# Example usage\n",
    "audio_file_path = \"conversation.wav\"  # Replace with your audio file path\n",
    "output_file_path = \"transcription.txt\"  # Path for the output text file\n",
    "\n",
    "# Perform transcription\n",
    "transcription = transcribe_audio(audio_file_path)\n",
    "\n",
    "# Save transcription to a text file\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(transcription)\n",
    "\n",
    "print(f\"Transcription completed and saved to {output_file_path}\")\n",
    "\n",
    "!pip install openai\n",
    "\n",
    "%cd /content\n",
    "\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"\"  # Replace with your actual API key\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def split_into_chunks(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def classify_text(chunk: str, prompt: str) -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies text.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nText to classify:\\n{chunk}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def process_file(file_path: str, prompt: str) -> Tuple[List[str], List[str]]:\n",
    "    text = read_file(file_path)\n",
    "    chunks = split_into_chunks(text)\n",
    "\n",
    "    label1_chunks = []\n",
    "    label2_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        classification = classify_text(chunk, prompt)\n",
    "        if \"Label 1\" in classification:\n",
    "            label1_chunks.append(chunk)\n",
    "        elif \"Label 2\" in classification:\n",
    "            label2_chunks.append(chunk)\n",
    "\n",
    "    return label1_chunks, label2_chunks\n",
    "\n",
    "def save_results(label1_chunks: List[str], label2_chunks: List[str], output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'label1_text.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(label1_chunks))\n",
    "\n",
    "    with open(os.path.join(output_dir, 'label2_text.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(label2_chunks))\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = input(\"transcription.txt\")\n",
    "    prompt = input(\"please classify the female and male voice text and label them as male and female \")\n",
    "    output_dir = input(\"opt.txt\")\n",
    "\n",
    "    label1_chunks, label2_chunks = process_file(file_path, prompt)\n",
    "    save_results(label1_chunks, label2_chunks, output_dir)\n",
    "\n",
    "    print(f\"Classification complete. Results saved in {output_dir}\")\n",
    "\n",
    "!pip install openai==0.28\n",
    "\n",
    "!pip install openai nltk\n",
    "\n",
    "import openai\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"api key\"  # Replace with your actual API key\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "    prompt = f\"Classify the following sentence as either doctor's speech or patient's speech. Respond with only 'Patient:' followed by the sentence.\\n\\nSentence: {sentence}\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies speech in medical conversations.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def process_transcription(input_file, output_file):\n",
    "    # Read the transcription\n",
    "    transcription = read_file(input_file)\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(transcription)\n",
    "\n",
    "    # Classify each sentence\n",
    "    classified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        classification = classify_sentence(sentence)\n",
    "        classified_sentences.append(classification)\n",
    "\n",
    "    # Write the classified sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for sentence in classified_sentences:\n",
    "            file.write(f\"{sentence}\\n\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"transcription.txt\"\n",
    "    output_file = \"opt.txt\"\n",
    "\n",
    "    process_transcription(input_file, output_file)\n",
    "\n",
    "    print(f\"Classification complete. Results saved in {output_file}\")\n",
    "\n",
    "# import re\n",
    "# import nltk\n",
    "\n",
    "# # Download the punkt tokenizer for sentence splitting\n",
    "# nltk.download('punkt', quiet=True)\n",
    "\n",
    "# # Lists of keywords and phrases often used by patients and doctors\n",
    "# patient_keywords = [\n",
    "#     \"feel\", \"pain\", \"hurt\", \"worried\", \"concerned\", \"scared\", \"symptom\",\n",
    "#     \"problem\", \"issue\", \"medication\", \"medicine\", \"drug\", \"pill\",\n",
    "#     \"side effect\", \"family history\", \"allergy\", \"allergic\"\n",
    "# ]\n",
    "\n",
    "# doctor_keywords = [\n",
    "#     \"diagnosis\", \"treatment\", \"prescription\", \"recommend\", \"suggest\",\n",
    "#     \"test\", \"examination\", \"results\", \"condition\", \"prognosis\",\n",
    "#     \"follow-up\", \"specialist\", \"referral\", \"dose\", \"medical history\"\n",
    "# ]\n",
    "\n",
    "# question_words = [\"what\", \"how\", \"when\", \"where\", \"why\", \"do\", \"is\", \"are\", \"can\", \"could\"]\n",
    "\n",
    "# def is_question(sentence):\n",
    "#     return any(sentence.lower().startswith(word) for word in question_words) or sentence.endswith(\"?\")\n",
    "\n",
    "# def count_keywords(sentence, keyword_list):\n",
    "#     return sum(1 for keyword in keyword_list if keyword in sentence.lower())\n",
    "\n",
    "# def classify_sentence(sentence):\n",
    "#     patient_score = count_keywords(sentence, patient_keywords)\n",
    "#     doctor_score = count_keywords(sentence, doctor_keywords)\n",
    "\n",
    "#     # Doctors tend to ask more questions\n",
    "#     if is_question(sentence):\n",
    "#         doctor_score += 1\n",
    "\n",
    "#     # Patients often use first-person pronouns\n",
    "#     if re.search(r'\\b(i|me|my|mine)\\b', sentence.lower()):\n",
    "#         patient_score += 1\n",
    "\n",
    "#     # Doctors often use second-person pronouns\n",
    "#     if re.search(r'\\b(you|your|yours)\\b', sentence.lower()):\n",
    "#         doctor_score += 1\n",
    "\n",
    "#     if patient_score > doctor_score:\n",
    "#         return \"Patient: \" + sentence\n",
    "#     elif doctor_score > patient_score:\n",
    "#         return \"Doctor: \" + sentence\n",
    "#     else:\n",
    "#         return \"Unclassified: \" + sentence\n",
    "\n",
    "# def process_transcription(input_file, output_file):\n",
    "#     # Read the transcription\n",
    "#     with open(input_file, 'r', encoding='utf-8') as file:\n",
    "#         transcription = file.read()\n",
    "\n",
    "#     # Split into sentences\n",
    "#     sentences = nltk.sent_tokenize(transcription)\n",
    "\n",
    "#     # Classify each sentence\n",
    "#     classified_sentences = [classify_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "#     # Write the classified sentences to the output file\n",
    "#     with open(output_file, 'w', encoding='utf-8') as file:\n",
    "#         for sentence in classified_sentences:\n",
    "#             file.write(f\"{sentence}\\n\")\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_file = \"transcription.txt\"\n",
    "#     output_file = \"classified_conversation.txt\"\n",
    "\n",
    "#     process_transcription(input_file, output_file)\n",
    "\n",
    "#     print(f\"Classification complete. Results saved in {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
