{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AI system that can extract and summarize key legal information from complex legal documents**"
      ],
      "metadata": {
        "id": "fjDk0GaxcXWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJtIOJdLchPe",
        "outputId": "f096dc11-4bc6-4b00-9119-d298468e066a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1G_gxahcPCH",
        "outputId": "9fce1c9d-9244-480f-e902-19ddba7bcc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 150, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New York for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots.\n",
            "\n",
            "Key Entities:\n",
            "\n",
            "Key Clauses:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Add more preprocessing steps as needed\n",
        "    return text\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {ent.label_: ent.text for ent in doc.ents}\n",
        "    return entities\n",
        "\n",
        "def extract_key_clauses(text):\n",
        "    # Implement logic to identify key clauses\n",
        "    # This could involve keyword matching, machine learning, etc.\n",
        "    # For simplicity, let's just split by newlines for now\n",
        "    clauses = text.split('\\n')\n",
        "    return [clause for clause in clauses if len(clause) > 50]  # arbitrary length threshold\n",
        "\n",
        "def summarize_text(text, max_length=150):\n",
        "    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "def generate_report(text):\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    entities = extract_entities(preprocessed_text)\n",
        "    key_clauses = extract_key_clauses(preprocessed_text)\n",
        "    summary = summarize_text(preprocessed_text)\n",
        "\n",
        "    report = f\"Summary:\\n{summary}\\n\\n\"\n",
        "    report += \"Key Entities:\\n\"\n",
        "    for label, entity in entities.items():\n",
        "        report += f\"{label}: {entity}\\n\"\n",
        "    report += \"\\nKey Clauses:\\n\"\n",
        "    for clause in key_clauses[:5]:  # Limit to first 5 for brevity\n",
        "        report += f\"- {clause}\\n\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# Main execution\n",
        "pdf_path = \"R.pdf\"\n",
        "legal_text = extract_text_from_pdf(pdf_path)\n",
        "report = generate_report(legal_text)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmduyUaDcRzD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}